# -*- coding: utf-8 -*-
"""Copy of Main Copy of Movie _recommendation_ensemble_learning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eGPdunJIqrXK0hig_NC1zZAydQdDdICt

**Import Libraries and Load the Data
python**
"""

# Import necessary libraries
import numpy as np
import pandas as pd


from google.colab import drive
drive.mount('/content/drive')

# Load datasets
movies = pd.read_csv('/content/drive/MyDrive/Datasets of MRSEL/tmdb_5000_movies.csv/tmdb_5000_movies.csv')
credits = pd.read_csv('/content/drive/MyDrive/Datasets of MRSEL/tmdb_5000_credits.csv/tmdb_5000_credits.csv' )
# Merge datasets on the title column
movies = movies.merge(credits, on='title')

"""**Data Exploration**"""

# Display the first few rows of the dataset
movies.head(3)

# Count the unique values in 'original_language'
movies['original_language'].value_counts()

# Check dataset info
movies.info()

"""**Data Preprocessing: Selecting Important Columns**"""

# Select relevant columns
movies = movies[['movie_id', 'title', 'overview', 'genres', 'keywords', 'cast', 'crew']]

# Drop null values
movies.dropna(inplace=True)

# Check for duplicate rows
movies.duplicated().sum()

"""**Transforming JSON Fields
Genres and Keywords Conversion**
"""

import ast

# Function to extract 'name' values from dictionaries
def convert(obj):
    L = []
    for i in ast.literal_eval(obj):
        L.append(i['name'])
    return L

# Apply transformation
movies['genres'] = movies['genres'].apply(convert)
movies['keywords'] = movies['keywords'].apply(convert)

"""**Cast and Crew Conversion**"""

# Extract top 3 cast members
def convert3(obj):
    L = []
    count = 0
    for i in ast.literal_eval(obj):
        if count != 3:
            L.append(i['name'])
            count += 1
        else:
            break
    return L

movies['cast'] = movies['cast'].apply(convert3)

# Extract director name from crew
def fetch_dirc(obj):
    L = []
    for i in ast.literal_eval(obj):
        if i['job'] == 'Director':
            L.append(i['name'])
            break
    return L

movies['crew'] = movies['crew'].apply(fetch_dirc)

"""**Text Preprocessing: Tokenizing and Combining Columns**"""

# Convert overview into a list of words
movies['overview'] = movies['overview'].apply(lambda x: x.split())

# Remove spaces from genre, keywords, cast, and crew elements
for column in ['genres', 'keywords', 'cast', 'crew']:
    movies[column] = movies[column].apply(lambda x: [i.replace(" ", "") for i in x])

# Concatenate all relevant fields into a single 'tags' column
movies['tags'] = movies['overview'] + movies['genres'] + movies['keywords'] + movies['cast'] + movies['crew']

# Create a new DataFrame with selected columns
new_data = movies[['movie_id', 'title', 'tags']]

# Convert lists in 'tags' column to strings
new_data['tags'] = new_data['tags'].apply(lambda x: " ".join(x).lower())

"""**Stemming**"""

from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()

# Function to apply stemming
def stem(text):
    y = []
    for i in text.split():
        y.append(ps.stem(i))
    return " ".join(y)

new_data['tags'] = new_data['tags'].apply(stem)

"""**Vectorization: Bag of Words**"""

from sklearn.feature_extraction.text import CountVectorizer

# Convert text into vectors
cv = CountVectorizer(max_features=5000, stop_words='english')
vectors = cv.fit_transform(new_data['tags']).toarray()

# Check feature names
cv.get_feature_names_out()

"""**1.Cosine Similarity for Recommendations**"""

from sklearn.metrics.pairwise import cosine_similarity

# Compute similarity matrix
similarity = cosine_similarity(vectors)

# Function to recommend movies
def recommend(movie):
    if movie not in new_data['title'].values:
        return "Movie not found in the dataset. Please try another title."

    movie_index = new_data[new_data['title'] == movie].index[0]
    distances = similarity[movie_index]
    movies_list = sorted(list(enumerate(distances)), reverse=True, key=lambda x: x[1])[1:6]

    return [new_data.iloc[i[0]].title for i in movies_list]

# User input and calling the function
movie = input("Enter the movie title: ")
recommended_movies = recommend(movie)

if isinstance(recommended_movies, list):  # If recommendations are valid
    print("Recommended movies:")
    for title in recommended_movies:
        print(title)
else:
    print(recommended_movies)  # Prints the error message if the movie isn't found

"""**Advanced Techniques: TF-IDF**"""

from sklearn.feature_extraction.text import TfidfVectorizer

# TF-IDF Vectorization
tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(new_data['tags'])

# Cosine similarity using TF-IDF
cosine_sim_tfidf = cosine_similarity(tfidf_matrix)

# Function to recommend movies based on TF-IDF
def get_recommendations_tfidf(title):
    idx = new_data[new_data['title'] == title].index[0]
    sim_scores = list(enumerate(cosine_sim_tfidf[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:6]
    return [new_data.iloc[i[0]].title for i in sim_scores]

# Example usage
get_recommendations_tfidf('The Dark Knight')

"""**2.LSI (Latent Semantic Indexing) Model**"""

from sklearn.decomposition import TruncatedSVD
from sklearn.metrics.pairwise import cosine_similarity

# Apply SVD to reduce dimensionality of the TF-IDF matrix
svd = TruncatedSVD(n_components=100, random_state=42)
lsi_matrix = svd.fit_transform(tfidf_matrix)

# Calculate Cosine Similarity in the reduced LSI space
cosine_sim_lsi = cosine_similarity(lsi_matrix)

# Function to get movie recommendations using LSI
def get_recommendations_lsi(title, cosine_sim=cosine_sim_lsi):
    idx = new_data[new_data['title'] == title].index[0]
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:11]  # Get top 10 similar movies
    movie_indices = [i[0] for i in sim_scores]
    return new_data['title'].iloc[movie_indices]

"""**3.BM25 Model**"""

try:
    from rank_bm25 import BM25Okapi
except ImportError:
    import os
    os.system('pip install rank-bm25')  # Install dynamically if missing
    from rank_bm25 import BM25Okapi

# Assuming each entry in 'tags' is already a list of tags
tokenized_tags = new_data['tags'].apply(lambda x: x.split()).tolist() # Tokenizing the tags

# Build BM25 model
bm25 = BM25Okapi(tokenized_tags)

# Function to get BM25-based recommendations
def get_recommendations_bm25(title):
    idx = new_data[new_data['title'] == title].index[0]
    query = new_data['tags'].iloc[idx]  # Use the list directly
    scores = bm25.get_scores(query)
    sim_scores = list(enumerate(scores))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:11]  # Get top 10 similar movies
    movie_indices = [i[0] for i in sim_scores]
    return new_data['title'].iloc[movie_indices]

"""**4. Word2Vec Model**"""

import numpy as np
from gensim.models import Word2Vec
import os

model_path = "word2vec_movie_model.bin"

# Tokenize the 'tags' column before using it in Word2Vec
# This line was moved from the Jaccard Similarity section to here
new_data['tokenized_tags'] = new_data['tags'].apply(lambda x: x.split())

if os.path.exists(model_path):
    # Load the pre-trained model
    word2vec = Word2Vec.load(model_path)
else:
    # Train and save the model
    word2vec = Word2Vec(sentences=new_data['tokenized_tags'], vector_size=100, window=5, min_count=1, workers=4)
    word2vec.save(model_path)

# Create a movie vector by averaging word vectors from the tokenized tags
def get_movie_vector(movie_idx):
    words = new_data['tokenized_tags'].iloc[movie_idx]  # Use the new tokenized_tags column
    vectors = [word2vec.wv[word] for word in words if word in word2vec.wv]
    return np.mean(vectors, axis=0) if vectors else np.zeros(word2vec.vector_size)

# Calculate similarity between movie vectors
movie_vectors = np.array([get_movie_vector(idx) for idx in range(len(new_data))])
cosine_sim_word2vec = cosine_similarity(movie_vectors)

# Function to get Word2Vec-based recommendations
def get_recommendations_word2vec(title):
    idx = new_data[new_data['title'] == title].index[0]
    sim_scores = list(enumerate(cosine_sim_word2vec[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:11]  # Get top 10 similar movies
    movie_indices = [i[0] for i in sim_scores]
    return new_data['title'].iloc[movie_indices]

"""**5. Jaccard Similarity Model**"""

from sklearn.metrics import jaccard_score
from sklearn.preprocessing import MultiLabelBinarizer

# Tokenize the 'tags' column into individual words
new_data['tokenized_tags'] = new_data['tags'].apply(lambda x: x.split())

# Use MultiLabelBinarizer to transform tags into binary vectors
mlb = MultiLabelBinarizer()
binary_vectors = mlb.fit_transform(new_data['tokenized_tags'])

# Function to get Jaccard similarity between two movies
def get_jaccard_similarity(movie_idx1, movie_idx2):
    binary_vector1 = binary_vectors[movie_idx1]
    binary_vector2 = binary_vectors[movie_idx2]

    # Compute Jaccard similarity
    return jaccard_score(binary_vector1, binary_vector2)

# Function to get Jaccard-based recommendations
def get_recommendations_jaccard(title):
    idx = new_data[new_data['title'] == title].index[0]
    sim_scores = [(i, get_jaccard_similarity(idx, i)) for i in range(len(new_data))]
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:11]  # Get top 10 similar movies
    movie_indices = [i[0] for i in sim_scores]
    return new_data['title'].iloc[movie_indices]

"""**Ensemble Recommendation**"""

from collections import Counter

# Ensemble recommendation using multiple models from Code 1
def get_ensemble_recommendations(title):
    recommendations = []

    # Get recommendations from each model in Code 1
    recommendations += list(get_recommendations_tfidf(title))  # TF-IDF based recommendations
    recommendations += list(get_recommendations_lsi(title))   # LSI-based recommendations
    recommendations += list(get_recommendations_bm25(title))  # BM25-based recommendations
    recommendations += list(get_recommendations_word2vec(title))  # Word2Vec-based recommendations
    recommendations += list(get_recommendations_jaccard(title))  # Jaccard similarity-based recommendations

    # Majority voting to get the top 5 recommendations
    top_recommendations = Counter(recommendations).most_common(5)
    return [rec[0] for rec in top_recommendations]

# Example usage: Get ensemble-based recommendations
get_ensemble_recommendations("The Avengers")

"""**Optional: Save or Export Results**"""

# Save preprocessed data for future use
new_data.to_csv('preprocessed_movies.csv', index=False)

"""**SIMILARITY MATRICES**"""

import pickle
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import TruncatedSVD
!pip install rank_bm25
from rank_bm25 import BM25Okapi
from gensim.models import Word2Vec
from sklearn.preprocessing import MultiLabelBinarizer

# Load the preprocessed data containing the 'tags' column
movies = pd.read_csv('preprocessed_movies.csv')


# 1. **TF-IDF Cosine Similarity**
tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(movies['tags'])
similarity_tfidf = cosine_similarity(tfidf_matrix)
pickle.dump(similarity_tfidf, open('similarity_tfidf.pkl', 'wb'))

# 2. **LSI Similarity**
svd = TruncatedSVD(n_components=100, random_state=42)
lsi_matrix = svd.fit_transform(tfidf_matrix)
similarity_lsi = cosine_similarity(lsi_matrix)
pickle.dump(similarity_lsi, open('similarity_lsi.pkl', 'wb'))

# 3. **BM25 Similarity**
tokenized_tags = movies['tags'].apply(lambda x: x.split()).tolist()
bm25 = BM25Okapi(tokenized_tags)

# Calculate BM25 scores for all documents against all documents
bm25_scores = np.array([bm25.get_scores(query) for query in tokenized_tags])
similarity_bm25 = cosine_similarity(bm25_scores) # Calculate cosine similarity using bm25 scores
pickle.dump(similarity_bm25, open('similarity_bm25.pkl', 'wb'))

# 4. **Word2Vec Similarity**
word2vec = Word2Vec(sentences=tokenized_tags, vector_size=100, window=5, min_count=1, workers=4)
movie_vectors = np.array([np.mean([word2vec.wv[word] for word in words if word in word2vec.wv], axis=0) if words else np.zeros(100) for words in tokenized_tags])
similarity_word2vec = cosine_similarity(movie_vectors)
pickle.dump(similarity_word2vec, open('similarity_word2vec.pkl', 'wb'))

# 5. **Jaccard Similarity**
mlb = MultiLabelBinarizer()
binary_vectors = mlb.fit_transform(tokenized_tags)
similarity_jaccard = cosine_similarity(binary_vectors)
pickle.dump(similarity_jaccard, open('similarity_jaccard.pkl', 'wb'))

# **Download All Pickle Files**
from google.colab import files
for file in ["similarity_tfidf.pkl", "similarity_lsi.pkl", "similarity_bm25.pkl", "similarity_word2vec.pkl", "similarity_jaccard.pkl"]:
    files.download(file)